{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === PART 0: INSTALL DEPENDENCIES (KAGGLE) ============================\n# What this does:\n# - Installs auxiliary libs (NOT external grad-cam).\n# - PyTorch stays as preinstalled (Kaggle).\n# ======================================================================\n%pip install -q timm umap-learn\n# (optional) %pip install -q torchcam\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 1: IMPORTS & GLOBAL CONFIG (ROBUST + BUILT-IN GRAD-CAM) =====\n# What this does:\n# - Imports libs and sets paths/hyperparams (100+100 epochs, freezer OFF).\n# - Adds a built-in Grad-CAM fallback (no external install needed).\n# - Kaggle-safe: NUM_WORKERS=0 and throttled tqdm to avoid timeouts.\n# ======================================================================\nimport os, random, time, math, shutil\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid, save_image\n\nimport timm\n\nfrom sklearn.metrics import (classification_report, confusion_matrix, roc_curve, auc)\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.cm as mpl_cm\nfrom tqdm.auto import tqdm\n\n# ---------- Built-in Grad-CAM (pure PyTorch) ----------\ndef _overlay_cam_on_image_rgb(raw_rgb, grayscale_cam, alpha=0.35):\n    \"\"\"Blend a [H,W,3] RGB float (0..1) with a [H,W] CAM heatmap.\"\"\"\n    raw = np.asarray(raw_rgb, dtype=np.float32)\n    if raw.max() > 1.0: raw = raw / 255.0\n    gray = np.asarray(grayscale_cam, dtype=np.float32)\n    if gray.ndim == 3 and gray.shape[0] == 1: gray = gray[0]\n    gray = np.clip(gray, 0.0, 1.0)\n    heat = mpl_cm.get_cmap('jet')(gray)[..., :3]\n    out = (1 - alpha) * raw + alpha * heat\n    return (np.clip(out, 0, 1) * 255).astype(np.uint8)\n\nclass _SimpleGradCAM:\n    \"\"\"Minimal Grad-CAM for a single target layer.\"\"\"\n    def __init__(self, model, target_layer):\n        self.model, self.layer = model, target_layer\n        self.activations, self.gradients = None, None\n        self.h_fwd = self.layer.register_forward_hook(self._save_act)\n        if hasattr(self.layer, 'register_full_backward_hook'):\n            self.h_bwd = self.layer.register_full_backward_hook(self._save_grad)\n        else:\n            self.h_bwd = self.layer.register_backward_hook(self._save_grad_depr)\n    def _save_act(self, mod, inp, out): self.activations = out\n    def _save_grad(self, mod, gin, gout): self.gradients = gout[0]\n    def _save_grad_depr(self, mod, gin, gout): self.gradients = gout[0]\n    @torch.no_grad()\n    def _normalize(self, cam):\n        cam = torch.relu(cam)\n        cam_min = cam.amin(dim=(1,2), keepdim=True)\n        cam_max = cam.amax(dim=(1,2), keepdim=True)\n        return (cam - cam_min) / (cam_max - cam_min + 1e-6)\n    def __call__(self, input_tensor, class_idx=None):\n        self.model.zero_grad(set_to_none=True)\n        input_tensor.requires_grad_(True)\n        output = self.model(input_tensor)\n        if class_idx is None:\n            class_idx = output.argmax(1).item()\n        loss = output[:, class_idx].sum()\n        loss.backward()\n        grads, acts = self.gradients, self.activations\n        weights = grads.mean(dim=(2,3), keepdim=True)\n        cam = (weights * acts).sum(dim=1)\n        return self._normalize(cam).detach().cpu().numpy()\n\ndef make_cam_runner(model, target_layer):\n    engine = _SimpleGradCAM(model, target_layer)\n    def _runner(input_tensor, class_idx):\n        return engine(input_tensor, int(class_idx))[0]\n    return _runner\n\n# ------------------ Device/seed/paths/hparams -------------------------\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nIS_KAGGLE = os.path.exists(\"/kaggle\")\nRUN_NAME = \"moco_rice38_70_20_10\"\n\n# EDIT if your dataset folder differs:\nDATA_DIR = Path(\"/kaggle/input/riceds-original/Original\") if IS_KAGGLE else Path(\"riceds-original/Original\")\nWORK_DIR = Path(f\"/kaggle/working/runs/{RUN_NAME}\") if IS_KAGGLE else Path(f\"runs/{RUN_NAME}\")\nWORK_DIR.mkdir(parents=True, exist_ok=True)\n\nIMG_SIZE = 224\nBATCH_SIZE = 16\n\n# Kaggle-safe loader settings (fixes multiprocessing shutdown AssertionError)\nNUM_WORKERS = 0\nPIN_MEMORY = (DEVICE == 'cuda')\nPERSISTENT = False\n\nNUM_CLASSES = 38\n\n# MoCo pretrain + Supervised finetune epochs\nMOCO_EPOCHS = 100\nFINETUNE_EPOCHS = 100\n\n# Supervised training hparams\nBASE_LR = 3e-4\nWEIGHT_DECAY = 1e-4\nPATIENCE = 1000          # effectively off\nLABEL_SMOOTH = 0.05\nMIXUP_ALPHA = 0.2\nCUTMIX_ALPHA = 0.0\nTTA_TIMES = 4\n\n# MoCo-specific hparams\nPROJ_DIM = 256\nQUEUE_SIZE = 65536\nMOMENTUM = 0.999\nTEMP = 0.2\nSSL_LR = 0.2\nSSL_WD = 1e-4\nSSL_MOM = 0.9\n\n# Throttle tqdm to reduce Jupyter IO stalls\nTQDM_KW = dict(dynamic_ncols=True, leave=False, mininterval=10.0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 2: DATA SCAN & STRATIFIED SPLIT (70/10/20) ==================\n# What this does:\n# - Collects image paths/labels from class subfolders.\n# - Stratified split: 20% test from full; then 12.5% of remaining 80% as val ⇒ 10% absolute.\n# - Leaves 70% for train.\n# ======================================================================\nassert DATA_DIR.exists(), f\"Path not found: {DATA_DIR.resolve()}\"\n\nclasses = sorted([p.name for p in DATA_DIR.iterdir() if p.is_dir()])\nclass_to_idx = {c:i for i,c in enumerate(classes)}\nassert len(classes) == NUM_CLASSES, f\"Expected {NUM_CLASSES} classes, found {len(classes)}\"\n\nall_paths, all_labels = [], []\nfor c in classes:\n    for imgp in (DATA_DIR/c).glob(\"*\"):\n        if imgp.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"}:\n            all_paths.append(str(imgp))\n            all_labels.append(class_to_idx[c])\n\nall_paths = np.array(all_paths)\nall_labels = np.array(all_labels)\nprint(\"Total images:\", len(all_paths))\n\n# 20% test from full\nsss_outer = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\ntrainval_idx, test_idx = next(sss_outer.split(all_paths, all_labels))\nX_trainval, y_trainval = all_paths[trainval_idx], all_labels[trainval_idx]\nX_test,     y_test     = all_paths[test_idx],    all_labels[test_idx]\n\n# From remaining 80%, take 12.5% as val (=> 10% absolute)\nsss_inner = StratifiedShuffleSplit(n_splits=1, test_size=0.125, random_state=SEED)\ntrain_idx, val_idx = next(sss_inner.split(X_trainval, y_trainval))\nX_train, y_train = X_trainval[train_idx], y_trainval[train_idx]\nX_val,   y_val   = X_trainval[val_idx],   y_trainval[val_idx]\n\nprint(f\"Split -> train: {len(X_train)} | val: {len(X_val)} | test: {len(X_test)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 3: DATASETS, AUGS, DATALOADERS (MoCo + Supervised) ==========\n# What this does:\n# - MoCo two-view dataset on train split; labeled datasets for train/val/test.\n# - Strong augs for SSL & train; light augs for val/test.\n# - WeightedRandomSampler to mitigate class imbalance (for supervised).\n# - DataLoader settings avoid multiprocessing teardown errors on Kaggle.\n# ======================================================================\nclass LabeledDataset(Dataset):\n    def __init__(self, paths, labels, transform=None):\n        self.paths = list(paths); self.labels = list(labels); self.transform = transform\n    def __len__(self): return len(self.paths)\n    def __getitem__(self, idx):\n        p = self.paths[idx]; y = self.labels[idx]\n        img = Image.open(p).convert(\"RGB\")\n        if self.transform: img = self.transform(img)\n        return img, y, p\n\nclass TwoCropsBYOL(Dataset):\n    \"\"\"Returns two independently augmented views of the same image (x1, x2).\"\"\"\n    def __init__(self, paths, transform1, transform2):\n        self.paths = list(paths); self.t1 = transform1; self.t2 = transform2\n    def __len__(self): return len(self.paths)\n    def __getitem__(self, idx):\n        img = Image.open(self.paths[idx]).convert(\"RGB\")\n        return self.t1(img), self.t2(img)\n\n# SSL augs (MoCo v2-style strong augs)\nssl_view1 = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.2, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomApply([transforms.ColorJitter(0.4,0.4,0.4,0.1)], p=0.8),\n    transforms.RandomGrayscale(p=0.2),\n    transforms.GaussianBlur(kernel_size=21, sigma=(0.1, 2.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n])\nssl_view2 = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.2, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomApply([transforms.ColorJitter(0.2,0.2,0.2,0.1)], p=0.8),\n    transforms.RandomGrayscale(p=0.2),\n    transforms.GaussianBlur(kernel_size=21, sigma=(0.1, 2.0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n])\n\ntrain_tfms = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomApply([transforms.ColorJitter(0.2,0.2,0.2,0.1)], p=0.6),\n    transforms.RandomAutocontrast(p=0.3),\n    transforms.RandomAdjustSharpness(1.5, p=0.3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n])\n\ntest_tfms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n])\n\n# Datasets\nssl_ds   = TwoCropsBYOL(X_train, ssl_view1, ssl_view2)   # unlabeled SSL on train\ntrain_ds = LabeledDataset(X_train, y_train, train_tfms)\nval_ds   = LabeledDataset(X_val,   y_val,   test_tfms)\ntest_ds  = LabeledDataset(X_test,  y_test,  test_tfms)\n\n# Weighted sampler for supervised training\nclass_counts = Counter(y_train)\nweights = [1.0/class_counts[y] for y in y_train]\nsampler = torch.utils.data.WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\n# Loaders (NUM_WORKERS=0 & persistent=False to avoid teardown AssertionError)\nssl_loader   = DataLoader(ssl_ds,   batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, \n                          pin_memory=PIN_MEMORY, drop_last=True,  persistent_workers=PERSISTENT)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS, \n                          pin_memory=PIN_MEMORY, drop_last=True,  persistent_workers=PERSISTENT)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, \n                          pin_memory=PIN_MEMORY,                persistent_workers=PERSISTENT)\ntest_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, \n                          pin_memory=PIN_MEMORY,                persistent_workers=PERSISTENT)\n\n# (Optional) quick batch viz\ndef show_batch(n=24, title=\"Supervised Train samples (augmented)\"):\n    x, y, _ = next(iter(DataLoader(train_ds, batch_size=n, shuffle=True, num_workers=0)))\n    grid = make_grid(x[:n], nrow=6, normalize=True, value_range=(-2,2))\n    plt.figure(figsize=(10,10)); plt.imshow(grid.permute(1,2,0).cpu()); plt.axis('off'); plt.title(title); plt.show()\n\n# show_batch()  # uncomment if you want a grid\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 4A: MoCo PRETRAINING ON TRAIN SPLIT =========================\n# What this does:\n# - Implements MoCo v2 (query/key encoders, momentum update, queue, InfoNCE).\n# - Trains self-supervised for MOCO_EPOCHS on the 70% train split.\n# - Saves the query encoder weights to moco_encoder.pt for fine-tuning.\n# ======================================================================\nclass ProjectionMLP(nn.Module):\n    def __init__(self, in_dim, hid=2048, out_dim=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hid), nn.BatchNorm1d(hid), nn.ReLU(inplace=True),\n            nn.Linear(hid, out_dim), nn.BatchNorm1d(out_dim)\n        )\n    def forward(self, x): return self.net(x)\n\n@torch.no_grad()\ndef _update_momentum(q_model, k_model, m=MOMENTUM):\n    for p_q, p_k in zip(q_model.parameters(), k_model.parameters()):\n        p_k.data = p_k.data * m + p_q.data * (1. - m)\n\ndef _normalize(x): return F.normalize(x, dim=1)\n\nfeat_dim = 1280  # EfficientNet-B0 feature size\nproj_dim = PROJ_DIM\n\ndef make_encoder():\n    enc = timm.create_model('tf_efficientnet_b0_ns', pretrained=True, num_classes=0)  # features\n    return enc\n\nencoder_q = make_encoder().to(DEVICE)\nencoder_k = make_encoder().to(DEVICE)\nprojector_q = ProjectionMLP(feat_dim, 2048, proj_dim).to(DEVICE)\nprojector_k = ProjectionMLP(feat_dim, 2048, proj_dim).to(DEVICE)\n\n# Init key encoder with query encoder weights; freeze key side\nencoder_k.load_state_dict(encoder_q.state_dict(), strict=False)\nprojector_k.load_state_dict(projector_q.state_dict(), strict=False)\nfor p in list(encoder_k.parameters()) + list(projector_k.parameters()):\n    p.requires_grad = False\n\n# MoCo queue (features are column-major: [C, K] for fast matmul)\nqueue = torch.randn(proj_dim, QUEUE_SIZE, device=DEVICE)\nqueue = F.normalize(queue, dim=0)\nqueue_ptr = torch.zeros(1, dtype=torch.long, device=DEVICE)\n\n@torch.no_grad()\ndef _dequeue_and_enqueue(keys):\n    \"\"\"keys: [B, C] l2-normalized\"\"\"\n    global queue, queue_ptr\n    bsz = keys.shape[0]\n    ptr = int(queue_ptr.item())\n    queue[:, ptr:ptr + bsz] = keys.T\n    ptr = (ptr + bsz) % QUEUE_SIZE\n    queue_ptr[0] = ptr\n\nopt_ssl = torch.optim.SGD(\n    list(encoder_q.parameters()) + list(projector_q.parameters()),\n    lr=SSL_LR, momentum=SSL_MOM, weight_decay=SSL_WD\n)\nscaler_ssl = torch.cuda.amp.GradScaler(enabled=(DEVICE=='cuda'))\n\nprint(\"Starting MoCo pretraining...\")\nencoder_q.train(); projector_q.train()\nencoder_k.eval();  projector_k.eval()\nfor epoch in range(1, MOCO_EPOCHS+1):\n    running = 0.0\n    pbar = tqdm(ssl_loader, desc=f\"MoCo Epoch {epoch}/{MOCO_EPOCHS}\", **TQDM_KW)\n    for x1, x2 in pbar:\n        x1, x2 = x1.to(DEVICE), x2.to(DEVICE)\n        with torch.cuda.amp.autocast(enabled=(DEVICE=='cuda')):\n            # Query path\n            q_feat = encoder_q(x1)\n            if isinstance(q_feat, (tuple, list)): q_feat = q_feat[0]\n            q = _normalize(projector_q(q_feat.flatten(1)))\n\n            with torch.no_grad():\n                # Update key encoder via momentum BEFORE computing keys\n                _update_momentum(encoder_q, encoder_k, m=MOMENTUM)\n                _update_momentum(projector_q, projector_k, m=MOMENTUM)\n                k_feat = encoder_k(x2)\n                if isinstance(k_feat, (tuple, list)): k_feat = k_feat[0]\n                k = _normalize(projector_k(k_feat.flatten(1)))\n\n            # MoCo InfoNCE logits: [B, 1 + K]\n            l_pos = (q * k).sum(dim=1, keepdim=True)         # [B,1]\n            l_neg = torch.matmul(q, queue)                   # [B,K]\n            logits = torch.cat([l_pos, l_neg], dim=1) / TEMP\n            labels = torch.zeros(logits.size(0), dtype=torch.long, device=DEVICE)  # positives at index 0\n\n            loss = F.cross_entropy(logits, labels)\n\n        scaler_ssl.scale(loss).backward()\n        scaler_ssl.step(opt_ssl); scaler_ssl.update()\n        opt_ssl.zero_grad(set_to_none=True)\n\n        with torch.no_grad():\n            _dequeue_and_enqueue(k)\n\n        running += loss.item() * x1.size(0)\n\n    epoch_loss = running / len(ssl_ds)\n    print(f\"MoCo epoch {epoch}: loss={epoch_loss:.4f}\")\n\n# Save the query encoder weights for fine-tuning\nmoco_ckpt_path = WORK_DIR / \"moco_encoder.pt\"\ntorch.save({\"encoder\": encoder_q.state_dict()}, moco_ckpt_path)\nprint(\"Saved MoCo encoder to:\", moco_ckpt_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 4B: SUPERVISED MODEL + OPTIMIZER + SCHEDULER + EMA ==========\n# What this does:\n# - Builds EfficientNet-B0 classifier (38 classes).\n# - Loads MoCo-pretrained query encoder weights where names match.\n# - FREEZER OFF: all layers trainable.\n# - AdamW + cosine LR with warmup; label smoothing; EMA tracker.\n# ======================================================================\nclass EMA:\n    def __init__(self, model, decay=0.999):\n        self.decay = decay\n        self.shadow = {n:p.detach().clone() for n,p in model.named_parameters() if p.requires_grad}\n    def update(self, model):\n        for n,p in model.named_parameters():\n            if p.requires_grad:\n                self.shadow[n] = (1-self.decay)*p.detach() + self.decay*self.shadow[n]\n    def apply_to(self, model):\n        with torch.no_grad():\n            for n,p in model.named_parameters():\n                if p.requires_grad:\n                    p.data.copy_(self.shadow[n].data)\n\nmodel = timm.create_model('tf_efficientnet_b0_ns', pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)\n\n# Load MoCo (query) encoder weights\nckpt = torch.load(moco_ckpt_path, map_location=DEVICE)\nenc = ckpt[\"encoder\"]\nmodel_dict = model.state_dict()\nloadable = {k:v for k,v in enc.items() if k in model_dict and v.shape == model_dict[k].shape}\nmodel_dict.update(loadable)\nmodel.load_state_dict(model_dict)\nprint(f\"Loaded {len(loadable)} MoCo params into classifier backbone.\")\n\n# FREEZER OFF\nfor p in model.parameters(): p.requires_grad = True\nprint(\"All layers trainable.\")\n\n# Optimizer + cosine LR warmup\noptimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\ntotal_steps = FINETUNE_EPOCHS * max(1, len(train_loader))\nwarmup_steps = int(0.1 * total_steps)\ndef lr_fn(step):\n    if step < warmup_steps: return (step+1)/max(1,warmup_steps)\n    p = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n    return 0.5*(1 + math.cos(math.pi * p))\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_fn)\n\nscaler = torch.cuda.amp.GradScaler(enabled=(DEVICE=='cuda'))\ncriterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\nema = EMA(model, decay=0.999)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 5: MIXUP / METRICS HELPERS =================================\n# What this does:\n# - Provides MixUp utilities (regularization for small datasets).\n# - Accuracy convenience function for quick evaluation.\n# ======================================================================\ndef mixup_data(x, y, alpha):\n    if alpha <= 0: \n        return x, y.float(), 1.0, torch.arange(len(y))\n    lam = np.random.beta(alpha, alpha)\n    bs = x.size(0)\n    idx = torch.randperm(bs, device=x.device)\n    mixed_x = lam * x + (1 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, (y_a, y_b), lam, idx\n\ndef mixup_criterion(criterion, pred, targets, lam):\n    y_a, y_b = targets\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\ndef accuracy(output, target):\n    return (output.argmax(1) == target).float().mean().item()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 6: SUPERVISED TRAINING + VALIDATION + ROBUST CKPTS ==========\n# What this does:\n# - Trains for FINETUNE_EPOCHS with AMP, MixUp, EMA updates (FREEZER OFF).\n# - Validates each epoch on the 10% validation split.\n# - Saves 'last_model.pt' EVERY epoch; saves 'best_model.pt' on val improv.\n# - Plots loss curve + val acc curve.\n# ======================================================================\nbest_val = -1.0\nbest_path = WORK_DIR / \"best_model.pt\"\nlast_path = WORK_DIR / \"last_model.pt\"\nhistory = {\"train_loss\":[], \"val_loss\":[], \"val_acc\":[]}\nno_improve = 0\nglobal_step = 0\nCLIP_NORM = 1.0\n\nfor epoch in range(1, FINETUNE_EPOCHS+1):\n    model.train()\n    running = 0.0\n    pbar = tqdm(train_loader, desc=f\"FT Epoch {epoch}/{FINETUNE_EPOCHS}\", **TQDM_KW)\n    for xb, yb, _ in pbar:\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        with torch.cuda.amp.autocast(enabled=(DEVICE=='cuda')):\n            if MIXUP_ALPHA>0 or CUTMIX_ALPHA>0:\n                xb, targets, lam, _ = mixup_data(xb, yb, MIXUP_ALPHA if MIXUP_ALPHA>0 else CUTMIX_ALPHA)\n                out = model(xb)\n                loss = mixup_criterion(criterion, out, targets, lam)\n            else:\n                out = model(xb)\n                loss = criterion(out, yb)\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n        scaler.step(optimizer); scaler.update()\n        optimizer.zero_grad(set_to_none=True)\n        ema.update(model)\n        running += loss.item() * xb.size(0)\n        scheduler.step(); global_step += 1\n\n    train_loss = running / len(train_ds)\n    history[\"train_loss\"].append(train_loss)\n\n    # ---- Validation with EMA weights\n    model_ema_eval = timm.create_model('tf_efficientnet_b0_ns', pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)\n    model_ema_eval.load_state_dict(model.state_dict())\n    ema.apply_to(model_ema_eval)\n    model_ema_eval.eval()\n\n    val_loss, val_acc = 0.0, 0.0\n    with torch.no_grad():\n        for xb, yb, _ in val_loader:\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n            out = model_ema_eval(xb)\n            loss = criterion(out, yb)\n            val_loss += loss.item() * xb.size(0)\n            val_acc  += (out.argmax(1) == yb).float().sum().item()\n\n    val_loss /= len(val_ds)\n    val_acc  /= len(val_ds)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_acc\"].append(val_acc)\n\n    # Save LAST (EMA) every epoch\n    torch.save({\"model\": model_ema_eval.state_dict(),\n                \"classes\": classes,\n                \"cfg\": dict(NUM_CLASSES=NUM_CLASSES, IMG_SIZE=IMG_SIZE),\n                \"epoch\": epoch,\n                \"val_acc\": val_acc},\n               last_path)\n    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n    print(f\"  -> saved LAST to {last_path}\")\n\n    # Save BEST if improved\n    if val_acc > best_val:\n        best_val = val_acc\n        no_improve = 0\n        shutil.copy2(last_path, best_path)\n        print(f\"  -> updated BEST to {best_path} (val_acc={val_acc:.4f})\")\n    else:\n        no_improve += 1\n        if no_improve >= PATIENCE:\n            print(\"Early stopping.\")\n            break\n\n# Ensure BEST exists even if no improvement was recorded\nif not best_path.exists() and last_path.exists():\n    shutil.copy2(last_path, best_path)\n    print(f\"No best found; copied LAST -> BEST:\\n  {last_path} -> {best_path}\")\n\n# Plot curves\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1); plt.plot(history[\"train_loss\"], label=\"train\"); plt.plot(history[\"val_loss\"], label=\"val\"); plt.title(\"Loss\"); plt.legend()\nplt.subplot(1,2,2); plt.plot(history[\"val_acc\"], label=\"val acc\"); plt.title(\"Val Accuracy\"); plt.legend()\nplt.tight_layout(); plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 7: MICRO OVERFIT CHECK (OPTIONAL) ===========================\n# What this does:\n# - Trains a fresh model on 8 samples for ~60 steps.\n# - If it can memorize them (high acc), your training pipeline is sound.\n# ======================================================================\ndef tiny_overfit_steps(steps=60):\n    model_small = timm.create_model('tf_efficientnet_b0_ns', pretrained=True, num_classes=NUM_CLASSES).to(DEVICE)\n    for p in model_small.parameters(): p.requires_grad = True\n    optimizer = torch.optim.AdamW(model_small.parameters(), lr=1e-3)\n    crit = nn.CrossEntropyLoss()\n    model_small.train()\n    xb, yb, _ = next(iter(train_loader))\n    xb, yb = xb[:8].to(DEVICE), yb[:8].to(DEVICE)\n    for i in range(steps):\n        out = model_small(xb)\n        loss = crit(out, yb)\n        optimizer.zero_grad(); loss.backward(); optimizer.step()\n        if (i+1)%10==0:\n            acc = (out.argmax(1)==yb).float().mean().item()\n            print(f\"step {i+1}/{steps} loss {loss.item():.3f} acc {acc:.3f}\")\n\n# tiny_overfit_steps()  # uncomment to run once\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 8: TEST EVAL + REPORTS + ROC/AUC + CONFUSION MATRIX =========\n# What this does:\n# - Loads BEST checkpoint; if missing, falls back to LAST.\n# - Restores `classes` from ckpt if needed.\n# - Runs TTA inference, prints classification report, plots CM & ROC.\n# ======================================================================\nbest_path = WORK_DIR / \"best_model.pt\"\nlast_path = WORK_DIR / \"last_model.pt\"\nckpt_path = best_path if best_path.exists() else last_path\nassert ckpt_path.exists(), f\"Checkpoint not found.\\nLooked for:\\n  {best_path}\\n  {last_path}\\nRun Part 6 first.\"\n\nprint(f\"Loading checkpoint: {ckpt_path}\")\nckpt = torch.load(ckpt_path, map_location=DEVICE)\nif \"classes\" not in globals():\n    classes = ckpt.get(\"classes\", None)\n    assert classes is not None, \"Classes not found in checkpoint and not defined in session.\"\n    NUM_CLASSES = len(classes)\n\nmodel_best = timm.create_model('tf_efficientnet_b0_ns', pretrained=False, num_classes=NUM_CLASSES).to(DEVICE)\nmodel_best.load_state_dict(ckpt[\"model\"], strict=True)\nmodel_best.eval()\n\ndef predict_with_tta(model, loader, tta=TTA_TIMES):\n    all_probs, all_labels, all_paths = [], [], []\n    aug = transforms.Compose([\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.9,1.0)),\n    ])\n    base = test_tfms\n    for xb, yb, ps in tqdm(loader, desc=\"Testing\", **TQDM_KW):\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        with torch.no_grad():\n            logits_sum = 0\n            for t in range(tta):\n                if t == 0:\n                    x_aug = xb\n                else:\n                    imgs = []\n                    for timg in xb:\n                        img = transforms.ToPILImage()(\n                            (timg.cpu()*torch.tensor([0.229,0.224,0.225]).view(3,1,1)+\n                             torch.tensor([0.485,0.456,0.406]).view(3,1,1)).clamp(0,1)\n                        )\n                        img = aug(img); img = base(img); imgs.append(img)\n                    x_aug = torch.stack(imgs).to(DEVICE)\n                logits_sum += model(x_aug)\n            probs = (logits_sum/tta).softmax(1)\n\n        all_probs.append(probs.cpu())\n        all_labels.append(yb.cpu())\n        all_paths += list(ps)\n\n    return torch.cat(all_probs).numpy(), torch.cat(all_labels).numpy(), all_paths\n\ntest_probs, test_labels, test_paths = predict_with_tta(model_best, test_loader, tta=TTA_TIMES)\ntest_pred = test_probs.argmax(1)\n\nprint(\"\\nClassification report:\")\nprint(classification_report(test_labels, test_pred, target_names=classes, digits=3))\n\n# Confusion matrix (do NOT name it 'cm' to avoid colormap shadowing)\nconf_mat = confusion_matrix(test_labels, test_pred)\nplt.figure(figsize=(12,10))\nsns.heatmap(conf_mat, cmap=\"Blues\", annot=False, fmt=\"d\")\nplt.title(\"Confusion Matrix\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.show()\n\n# ROC / AUC (one-vs-rest)\ny_true_bin = label_binarize(test_labels, classes=list(range(NUM_CLASSES)))\nfpr, tpr, roc_auc = {}, {}, {}\nfor i in range(NUM_CLASSES):\n    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:,i], test_probs[:,i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Micro/macro AUC\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), test_probs.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\nmacro_auc = np.mean([roc_auc[i] for i in range(NUM_CLASSES)])\nprint(f\"Macro AUC: {macro_auc:.4f} | Micro AUC: {roc_auc['micro']:.4f}\")\n\n# Plot micro + top-5 per-class by AUC\nplt.figure(figsize=(8,6))\nplt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f\"micro-average ROC (AUC = {roc_auc['micro']:.3f})\")\ntop5 = np.argsort([-roc_auc[i] for i in range(NUM_CLASSES)])[:5]\nfor i in top5:\n    plt.plot(fpr[i], tpr[i], lw=1, label=f\"{classes[i]} (AUC={roc_auc[i]:.3f})\")\nplt.plot([0,1],[0,1],'--', lw=1, color='gray')\nplt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curves\"); plt.legend(); plt.grid(alpha=0.2); plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 9: GRAD-CAM VISUAL EXPLANATIONS (SAME AS BYOL PART 9) =======\n# What this does:\n# - Uses the built-in Grad-CAM from Part 1 (no external lib).\n# - Shows overlays for top confident corrects and mistakes.\n# - Includes a hotfix to delete any accidental global 'cm' variable.\n# ======================================================================\nif 'cm' in globals():\n    try: del cm\n    except Exception: pass\n\ntarget_layer = model_best.conv_head if hasattr(model_best, \"conv_head\") else list(model_best.modules())[-1]\ncam_runner = make_cam_runner(model_best, target_layer)\nprint(\"Grad-CAM ready on layer:\", type(target_layer).__name__)\n\ndef pick_examples(n=3):\n    \"\"\"Return indices of n most-confident corrects and n most-confident mistakes.\"\"\"\n    conf = test_probs.max(1)\n    correct_mask = (test_pred == test_labels)\n    wrong_mask = ~correct_mask\n    idx_correct_pool = np.where(correct_mask)[0]\n    idx_wrong_pool   = np.where(wrong_mask)[0]\n    idx_correct_sorted = idx_correct_pool[np.argsort(-conf[idx_correct_pool])][:n]\n    idx_wrong_sorted   = idx_wrong_pool[np.argsort(-conf[idx_wrong_pool])][:n]\n    return list(idx_correct_sorted), list(idx_wrong_sorted)\n\ngood_ids, bad_ids = pick_examples(n=3)\n\ndef show_cam(idx_list, title):\n    \"\"\"Render Grad-CAM overlays for a list of test indices.\"\"\"\n    if not idx_list:\n        print(f\"{title}: no examples to show\"); \n        return\n    plt.figure(figsize=(12,4))\n    for j, idx in enumerate(idx_list):\n        p = test_paths[idx]\n        raw = np.array(Image.open(p).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))).astype(np.float32) / 255.0\n        x_cam = test_tfms(Image.fromarray((raw*255).astype(np.uint8))).unsqueeze(0).to(DEVICE)\n        cls = int(test_pred[idx])\n        grayscale = cam_runner(x_cam, cls)  # [H,W] 0..1\n        vis = _overlay_cam_on_image_rgb(raw, grayscale, alpha=0.35)\n        plt.subplot(1, len(idx_list), j+1)\n        plt.imshow(vis); plt.axis('off')\n        plt.title(f\"pred={classes[test_pred[idx]]}\\ntrue={classes[test_labels[idx]]}\")\n    plt.suptitle(title); plt.tight_layout(); plt.show()\n\nshow_cam(good_ids, \"Grad-CAM — confident corrects\")\nshow_cam(bad_ids,  \"Grad-CAM — confident mistakes\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 10: CLUSTERING & 2D VISUALIZATION ===========================\n# What this does:\n# - Extracts penultimate-layer embeddings from the trained classifier.\n# - KMeans clustering (k=38) with ARI/NMI/Silhouette metrics.\n# - t-SNE plots: colored by TRUE labels and by predicted clusters.\n# ======================================================================\nckpt_path = (WORK_DIR/\"best_model.pt\") if (WORK_DIR/\"best_model.pt\").exists() else (WORK_DIR/\"last_model.pt\")\nckpt = torch.load(ckpt_path, map_location=DEVICE)\n\nfeature_model = timm.create_model('tf_efficientnet_b0_ns', pretrained=False, num_classes=NUM_CLASSES)\nfeature_model.load_state_dict(ckpt[\"model\"])\nfeature_model.classifier = nn.Identity()  # output embeddings\nfeature_model = feature_model.to(DEVICE).eval()\n\ndef extract_features(loader):\n    feats, labels, paths = [], [], []\n    with torch.no_grad():\n        for xb, yb, ps in tqdm(loader, desc=\"Extract feats\", **TQDM_KW):\n            xb = xb.to(DEVICE)\n            f = feature_model(xb)\n            feats.append(f.cpu()); labels.append(yb); paths += list(ps)\n    return torch.cat(feats).numpy(), torch.cat(labels).numpy(), paths\n\nfeats_test, labels_test, _ = extract_features(test_loader)\n\nkmeans = KMeans(n_clusters=NUM_CLASSES, n_init=20, random_state=SEED)\nclusters = kmeans.fit_predict(feats_test)\n\nari = adjusted_rand_score(labels_test, clusters)\nnmi = normalized_mutual_info_score(labels_test, clusters)\nsil = silhouette_score(feats_test, clusters, sample_size=min(1000, len(feats_test)))\nprint(f\"Clustering -> ARI: {ari:.3f} | NMI: {nmi:.3f} | Silhouette: {sil:.3f}\")\n\ntsne = TSNE(n_components=2, init='pca', random_state=SEED, perplexity=min(30, max(5, len(feats_test)//5)))\nZ = tsne.fit_transform(feats_test)\n\nplt.figure(figsize=(9,7))\nplt.scatter(Z[:,0], Z[:,1], c=labels_test, s=12, cmap='tab20')\nplt.title(\"t-SNE (TRUE labels)\"); plt.axis('off'); plt.show()\n\nplt.figure(figsize=(9,7))\nplt.scatter(Z[:,0], Z[:,1], c=clusters, s=12, cmap='tab20')\nplt.title(\"t-SNE (KMeans clusters)\"); plt.axis('off'); plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 11: SAVE PREDICTIONS + ERROR GALLERY ========================\n# What this does:\n# - Writes per-image results (path/true/pred/confidence) from TEST to CSV.\n# - Saves a montage of the most-confident mistakes for quick QA.\n# ======================================================================\ndf = pd.DataFrame({\n    \"path\": test_paths,\n    \"true\": [classes[i] for i in test_labels],\n    \"pred\": [classes[i] for i in test_pred],\n    \"conf\": test_probs.max(1)\n})\ncsv_path = WORK_DIR/\"test_predictions.csv\"\ndf.to_csv(csv_path, index=False)\nprint(\"Saved:\", csv_path)\n\ndef save_error_montage(df, k=25, out=WORK_DIR/\"top_errors.jpg\"):\n    df_err = df[df.true!=df.pred].sort_values(\"conf\", ascending=False).head(k)\n    if df_err.empty:\n        print(\"No errors to show.\"); return\n    tiles = []\n    for p, t, pr, c in df_err[[\"path\",\"true\",\"pred\",\"conf\"]].values:\n        im = Image.open(p).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n        tiles.append(transforms.ToTensor()(im))\n    grid = make_grid(torch.stack(tiles), nrow=5)\n    save_image(grid, out)\n    print(\"Saved montage:\", out)\n\nsave_error_montage(df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === PART 12: SINGLE-IMAGE INFERENCE + GRAD-CAM (ROBUST) ==============\n# What this does:\n# - Runs a single image with optional TTA and prints top-k predictions.\n# - If true label is given, reports correctness.\n# - Saves a Grad-CAM overlay using the built-in fallback (same as BYOL).\n# ======================================================================\nfrom typing import Optional, Union\n\ndef infer_single_image(\n    image_path: str,\n    model=None,\n    topk: int = 5,\n    tta: int = TTA_TIMES,\n    true_label: Optional[Union[int, str]] = None,\n    save_cam_path: Optional[str] = None,\n):\n    assert os.path.exists(image_path), f\"Image not found: {image_path}\"\n    if model is None:\n        model = model_best\n    model.eval()\n\n    base_pil = Image.open(image_path).convert(\"RGB\")\n    aug = transforms.Compose([\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.9,1.0)),\n    ])\n\n    xs = []\n    for t in range(max(1, tta)):\n        pil = base_pil if t == 0 else aug(base_pil)\n        xs.append(test_tfms(pil))\n    xb = torch.stack(xs, dim=0).to(DEVICE)\n\n    with torch.no_grad():\n        logits = model(xb)\n        probs = logits.softmax(1).mean(0)\n\n    confs, idxs = torch.topk(probs, k=min(topk, NUM_CLASSES))\n    confs = confs.cpu().numpy(); idxs = idxs.cpu().numpy()\n    preds = [(classes[i], float(c)) for i, c in zip(idxs, confs)]\n\n    is_correct = None\n    if true_label is not None:\n        if isinstance(true_label, str):\n            assert true_label in classes, f\"Unknown class name: {true_label}\"\n            true_idx = classes.index(true_label)\n        else:\n            true_idx = int(true_label)\n        is_correct = (idxs[0] == true_idx)\n\n    try:\n        tgt_layer = model.conv_head if hasattr(model, \"conv_head\") else list(model.modules())[-1]\n        cam_runner = make_cam_runner(model, tgt_layer)\n        x_cam = test_tfms(base_pil).unsqueeze(0).to(DEVICE)\n        grayscale = cam_runner(x_cam, int(idxs[0]))  # top-1 class\n        raw = np.array(base_pil.resize((IMG_SIZE, IMG_SIZE))).astype(np.float32)/255.0\n        cam_vis = _overlay_cam_on_image_rgb(raw, grayscale, 0.35)\n        if save_cam_path is None:\n            save_cam_path = str(WORK_DIR / \"single_image_gradcam.jpg\")\n        Image.fromarray(cam_vis).save(save_cam_path)\n        cam_msg = f\"Grad-CAM saved to: {save_cam_path}\"\n    except Exception as e:\n        cam_msg = f\"Grad-CAM failed: {e}\"\n\n    print(f\"\\nSingle-image prediction for: {image_path}\")\n    for rank, (name, conf) in enumerate(preds, 1):\n        print(f\"  {rank:>2}. {name:>20s}  conf={conf:.4f}\")\n    if is_correct is not None:\n        print(f\"Correct? {is_correct} (true label = {classes[true_idx]})\")\n    print(cam_msg)\n\n    return {\n        \"image\": image_path,\n        \"topk\": preds,\n        \"pred_idx\": int(idxs[0]),\n        \"pred_class\": classes[int(idxs[0])],\n        \"pred_conf\": float(confs[0]),\n        \"is_correct\": is_correct,\n        \"cam_path\": save_cam_path,\n    }\n\n# Example:\n# result = infer_single_image(\n#     image_path=\"/kaggle/input/riceds-original/Original/<class>/<image>.jpg\",\n#     model=model_best,\n#     topk=5,\n#     tta=4,\n#     true_label=\"<class>\",\n# )\n# print(result)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}